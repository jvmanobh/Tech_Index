{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from os import mkdir\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from sklearn.model_selection import RandomizedSearchCV, ParameterSampler, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "import pyodbc\n",
    "import urllib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from tools import get_connection, entitle, fill, permutation_importances, scores\n",
    "import tree_explainer\n",
    "from datetime import datetime\n",
    "from scipy.stats import geom, uniform, randint\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from subprocess import call\n",
    "\n",
    "#For Space issue\n",
    "import os\n",
    "os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of models etc.\n",
    "folder = '/home/jovyan/Tech_Index/general_models/'\n",
    "\n",
    "# Location to output CSV files\n",
    "outfolder = folder\n",
    "\n",
    "conn_str='Driver={ODBC Driver 17 for SQL Server};Server=uschwsql1056d;Database=DEV_ANALYTICS;Trusted_Connection=yes;'\n",
    "\n",
    "# Set up a SQL conection\n",
    "engine = get_connection()\n",
    "\n",
    "#months to consider for training dataset\n",
    "m_cons = 12\n",
    "\n",
    "# Number of new hyperparameter combinations to check for each model:\n",
    "gridsize = 1    # just testing; will change later\n",
    "\n",
    "# Maximum number of data points allowed in a country's \n",
    "# training data (for the sake of memory and speed)\n",
    "maxsize = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for converting SQL feature names into plain English...\n",
    "def feature_english(f):\n",
    "    f = f.replace('SICMAJORGRPCD', 'vertical number (tricky variable)')\n",
    "    f = f.replace('SMBSHARE', 'proportion of resellers classified SMB')\n",
    "    f = f.replace('TRANSCOUNT', 'number of transactions X')\n",
    "    f = f.replace('PRODUCTCOUNT', 'number of distinct products transacted X')\n",
    "    f = f.replace('BCNCOUNT', 'number of distinct resellers dealt with X')\n",
    "    f = f.replace('EXTENDEDSALES', 'revenue X')\n",
    "    f = f.replace('DAYSSINCELASTTRANS', 'time passed since the last transaction')\n",
    "    f = f.replace('EmployeesTotalFinal', 'seat size')\n",
    "    f = f.replace('QMONTH', 'month within the quarter')\n",
    "    f = f.replace('MONTH', 'month of the year')\n",
    "    f = f.replace('QUARTER', 'quarter of the year')\n",
    "    f = f.replace('X1M', 'in the past month')\n",
    "    f = f.replace('X3M', 'in the past 3 months')\n",
    "    f = f.replace('X6M', 'in the past 6 months')\n",
    "    f = f.replace('X2Q', '2 quarters ago')\n",
    "    f = f.replace('X3Q', '3 quarters ago')\n",
    "    f = f.replace('X4Q', '4 quarters ago')   # should\n",
    "    f = f.replace('X5Q', '5 quarters ago')   # have\n",
    "    f = f.replace('X6Q', '6 quarters ago')   # used\n",
    "    f = f.replace('X7Q', '7 quarters ago')   # regex\n",
    "    f = f.replace('X8Q', '8 quarters ago')\n",
    "    f = f.replace('X', 'in the past year')\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************\n",
      "*                                                                                  *\n",
      "*  Measuring last month's performance and building new standardizing functions...  *\n",
      "*                                                                                  *\n",
      "************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:50<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************\n",
      "*                                                           *\n",
      "*  Checking correctness of last month's TECHINDEX_VALUE...  *\n",
      "*                                                           *\n",
      "*************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# Get the CALMONTHID of the most recent labeled predictions\n",
    "calmon = pd.read_sql(\n",
    "    '''\n",
    "    select max(CALMONTHID)\n",
    "    from DEV_ANALYTICS.dbo.TECHINDEX\n",
    "    where [LEVEL] = 'GENERAL'\n",
    "    and LABEL is not NULL\n",
    "    ''',\n",
    "    conn\n",
    ").iloc[0,0]\n",
    "\n",
    "\n",
    "\n",
    "# Get a list of all COMPANYCD\n",
    "ccd = pd.read_sql(\n",
    "    '''\n",
    "    select COMPANYCD\n",
    "    from DEV_ANALYTICS.DBO.TECHINDEX   \n",
    "    where [LEVEL] = 'GENERAL' and CALMONTHID = %d\n",
    "    group by COMPANYCD\n",
    "    order by COMPANYCD\n",
    "    ''' % calmon,\n",
    "    conn\n",
    ")\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Graph size parameters\n",
    "plt.rcParams['figure.figsize'] = 10, 15\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "entitle(\"Measuring last month's performance and building new standardizing functions...\")\n",
    "\n",
    "# Loop over all COMPANYCD\n",
    "for i in tqdm(range(len(ccd))):\n",
    "\n",
    "    COMPANYCD = ccd.iloc[i].COMPANYCD\n",
    "    \n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    # Get last month's outcomes\n",
    "    results = pd.read_sql(\n",
    "        \"SELECT RAWSCORE as pred, label as truth \" +\n",
    "        \"FROM DEV_ANALYTICS.dbo.TECHINDEX \" +\n",
    "        \"where companycd = '\" + COMPANYCD +\n",
    "        \"' and [LEVEL] = 'GENERAL\" +\n",
    "        \"' and CALMONTHID = \" + str(calmon) +\n",
    "        \";\", \n",
    "        conn\n",
    "    )\n",
    "    conn.close()\n",
    "\n",
    "    # Skip if COMPANYCD doesn't have both positives and negatives\n",
    "    if len(set(results.truth)) < 2: \n",
    "        continue\n",
    "\n",
    "        \n",
    "    ################################################\n",
    "    #      FIRST PLOT (UPPER LEFT): AUC CURVE      #\n",
    "    ################################################\n",
    "    plt.subplot(3,2,1)\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(results.truth, results.pred)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', c = 'black', lw = .5)\n",
    "    plt.plot(fpr, tpr, c='red', lw = 3)\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title(\n",
    "        'ROC Curve (AUC %.4f)' % \n",
    "        (metrics.roc_auc_score(results.truth, results.pred))\n",
    "    )\n",
    "\n",
    "    \n",
    "    ###########################################################\n",
    "    #    SECOND PLOT (UPPER RIGHT): PRECISION-RECALL CURVE    #\n",
    "    ###########################################################\n",
    "    plt.subplot(3,2,2)\n",
    "\n",
    "    fpr, tpr, _ = metrics.precision_recall_curve(results.truth, results.pred)\n",
    "    precision = metrics.precision_score(results.truth, results.pred > .3)\n",
    "    recall = metrics.recall_score(results.truth, results.pred > .3)\n",
    "    plt.plot(fpr, tpr, c='red', lw = 3, zorder = 10)\n",
    "    plt.grid()\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Precision & Recall')\n",
    "\n",
    "\n",
    "    #######################################################\n",
    "    #        THIRD PLOT (MIDDLE LEFT): GAIN CHART         #\n",
    "    #######################################################\n",
    "    plt.subplot(3,2,3)\n",
    "\n",
    "    results = results.sort_values('pred', ascending = False)\n",
    "    results['rand'] = results.sample(frac = 1).truth.values\n",
    "    results['wiz'] = results.sort_values('truth', ascending = False).truth.values\n",
    "    x = np.linspace(0, 1, 51 if len(results) >= 500 else 21)\n",
    "    \n",
    "    y_t = results.truth.sum()\n",
    "    y_r = []\n",
    "    y_m = []\n",
    "    y_w = []\n",
    "    for p in x:\n",
    "        y_r.append(results.head(int(len(results)*p)).rand.sum()/y_t)\n",
    "        y_m.append(results.head(int(len(results)*p)).truth.sum()/y_t)\n",
    "        y_w.append(results.head(int(len(results)*p)).wiz.sum()/y_t)\n",
    "\n",
    "    y_r, y_m, y_w = fill(y_r), fill(y_m), fill(y_w)\n",
    "        \n",
    "    plt.plot(x,y_r, lw = 3)\n",
    "    plt.plot(x,y_m, lw = 3)\n",
    "    plt.plot(x,y_w, lw = 3)\n",
    "    plt.xlabel('% from top')\n",
    "    plt.ylabel('% of all positives')\n",
    "    vals = plt.gca().get_xticks()\n",
    "    plt.gca().set_xticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "    vals = plt.gca().get_yticks()\n",
    "    plt.gca().set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "    plt.title('Cumulative Gain')\n",
    "    plt.legend(['Random', 'Model', 'Perfect'])\n",
    "\n",
    "\n",
    "    ######################################################\n",
    "    #       FOURTH PLOT (MIDDLE RIGHT): K-S CHART        #\n",
    "    ######################################################\n",
    "    plt.subplot(3,2,4)\n",
    "\n",
    "    y_f = (results.truth == 0).sum()\n",
    "    y_n = []\n",
    "    for p in x:\n",
    "        y_n.append((results.head(int(len(results)*p)).truth == 0).sum()/y_f)\n",
    "\n",
    "    y_n = fill(y_n)\n",
    "\n",
    "    KS = [y_m[i] - y_n[i] for i in range(len(x))]\n",
    "    KSi = KS.index(max(KS))\n",
    "\n",
    "    plt.plot(x,y_m, c='blue', lw = 3, zorder = 10)\n",
    "    plt.plot(x,y_n, c='red', lw = 3, zorder = 10)\n",
    "    plt.plot([x[KSi],x[KSi]], [y_n[KSi], y_m[KSi]], c='gray', lw = 5)\n",
    "    plt.text(x[KSi] + .01, (y_n[KSi] + y_m[KSi])/2, 'KS: %.2f' % max(KS), zorder = 6969)\n",
    "\n",
    "    plt.xlabel('% from top')\n",
    "    vals = plt.gca().get_xticks()\n",
    "    plt.gca().set_xticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "    vals = plt.gca().get_yticks()\n",
    "    plt.gca().set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "\n",
    "    plt.title('Kolomogorov-Smirnov')\n",
    "    plt.legend(['% of all positives', '% of all negatives'])\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    #    FIFTH PLOT (BOTTOM LEFT): CUMULATIVE LIFT CHART    #\n",
    "    #       AND CREATION OF STANDARDIZING FUNCTION!!        #\n",
    "    #########################################################\n",
    "    plt.subplot(3,2,5)\n",
    "  \n",
    "    x = np.concatenate((\n",
    "        np.linspace(0, x[1], 51 if len(results) >= 500 else 21), \n",
    "        x[2:]\n",
    "    ))\n",
    "    \n",
    "    y_l = []\n",
    "    for p in x:\n",
    "        y_l.append(\n",
    "            results.head(int(len(results)*p)).truth.mean() \n",
    "            / \n",
    "            results.truth.mean() - 1\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        ix = min([\n",
    "            i for i in range(len(x) - 1) \n",
    "            if (\n",
    "                int(len(results)*x[i]) >= 20\n",
    "                or x[i] >= .02\n",
    "            )\n",
    "            and y_l[i] > 0\n",
    "        ])\n",
    "        x = x[ix:]\n",
    "        y_l = y_l[ix:]    \n",
    "\n",
    "        hull = ConvexHull([[x[i], y_l[i]] for i in range(len(x))])\n",
    "\n",
    "        ihull = np.roll(\n",
    "            hull.vertices,\n",
    "            -list(hull.vertices).index(0)\n",
    "        )\n",
    "\n",
    "        ihull = ihull[:list(ihull).index(max(ihull))+1]\n",
    "\n",
    "        xhull = [x[i] for i in ihull]\n",
    "        yhull = [y_l[i] for i in ihull]\n",
    "\n",
    "        spf = UnivariateSpline(xhull, yhull, k = 1, s = 0, ext = 'const')\n",
    "\n",
    "        with open(folder + '/spf_' + COMPANYCD + '.pkl', 'wb') as f:\n",
    "            pickle.dump(spf, f)\n",
    "\n",
    "        xplot = np.linspace(0, 1, 1000)\n",
    "        plt.plot(xplot, spf(xplot), lw = 2, c='red', zorder = 42)\n",
    "        spf_done = True\n",
    "        \n",
    "    except:\n",
    "        spf_done = False\n",
    "\n",
    "\n",
    "    plt.plot(x, y_l, marker = 'o', linewidth = 0, zorder = 9001, c = 'blue')\n",
    "    plt.plot(x, y_l, linewidth = 1, c = 'blue')\n",
    "    plt.xlabel('% from top')\n",
    "    plt.ylabel('Cumulative lift (minus 1)')\n",
    "    vals = plt.gca().get_xticks()\n",
    "    plt.gca().set_xticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "    plt.title('Cumulative lift')\n",
    "    if spf_done:\n",
    "        plt.legend(['Standardizer', 'Tested lift'])\n",
    "        \n",
    "        \n",
    "    #####################################################\n",
    "    #    SIXTH PLOT (BOTTOM RIGHT): LOCAL LIFT CHART    #\n",
    "    #####################################################\n",
    "    plt.subplot(3,2,6)\n",
    "\n",
    "    x = np.linspace(0, 1, 21 if len(results) >= 500 else 11)\n",
    "    \n",
    "    y_r = []\n",
    "    y_l = []\n",
    "    dx = x[1] - x[0]\n",
    "    for p in x[1:]:\n",
    "        y_r.append(1)\n",
    "        y_l.append(\n",
    "            results.iloc[\n",
    "                int(len(results)*(p-dx)):int(len(results)*p)\n",
    "            ].truth.mean() \n",
    "            / results.rand.mean()\n",
    "        )\n",
    "\n",
    "    Qi = min([i for i in range(len(x)-1) if y_l[i] < y_r[i]]) - 1\n",
    "\n",
    "    plt.plot(x[1:],y_r, lw = 3)\n",
    "    plt.plot(x[1:],y_l, lw = 3)\n",
    "\n",
    "    if y_l[Qi] > y_l[Qi+1]:\n",
    "        Qx = x[Qi+2] \n",
    "        Qx -= (y_l[Qi+1] - 1) * (x[Qi+2] - x[Qi+1])/(y_l[Qi+1] - y_l[Qi])\n",
    "    else:\n",
    "        Qx = x[Qi+1]\n",
    "   \n",
    "    plt.scatter([Qx], [1], c='gray', zorder = 9001)\n",
    "    plt.text(Qx+.01, 1.1, '{:3.0f}%'.format(Qx*100))\n",
    "    vals = plt.gca().get_xticks()\n",
    "    plt.gca().set_xticklabels(['{:3.0f}%'.format((1-x)*100) for x in vals])\n",
    "    plt.xlabel('Percentile')\n",
    "    plt.ylabel('Lift at quantile')\n",
    "    plt.title('Lift')\n",
    "    plt.legend(['Random', 'Model'])\n",
    "\n",
    "    plt.suptitle('COMPANYCD ' + COMPANYCD, y = .95)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=.9)\n",
    "    plt.savefig(\n",
    "        folder + '/post_' + COMPANYCD + '.png', \n",
    "        facecolor = 'w', \n",
    "        transparent = False\n",
    "    )\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# New graph size parameters\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "entitle(\"Checking correctness of last month's TECHINDEX_VALUE...\")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "# Get last month's results \n",
    "df = pd.read_sql(\n",
    "    \"SELECT COMPANYCD, TECHINDEX_VALUE as impr, LABEL as truth \" +\n",
    "    \"FROM DEV_ANALYTICS.dbo.TECHINDEX \" +\n",
    "    \"where CALMONTHID = \" + str(calmon) +\n",
    "    \" and [LEVEL] = 'GENERAL\" + \"';\", \n",
    "    conn\n",
    ")\n",
    "conn.close()\n",
    "\n",
    "# Compare Tech Index value (AKA promised improvement) to actual improvement\n",
    "df_ = df.groupby([\n",
    "    df.COMPANYCD,\n",
    "    -(df.impr.apply(np.floor).astype(int))\n",
    "]).agg({\n",
    "    'impr'  : 'count',\n",
    "    'truth' : 'sum'\n",
    "})\n",
    "df_.columns = ['tot', 'pos']\n",
    "df_ = df_.reset_index()\n",
    "df_[['tot','pos']] = df_.groupby('COMPANYCD')[['tot','pos']].transform(pd.Series.cumsum)    \n",
    "df_ = df_.reset_index().join(\n",
    "    df.groupby('COMPANYCD').mean().truth,\n",
    "    on = 'COMPANYCD'\n",
    ")\n",
    "df_['Min Tech Index value'] = -(df_.impr)\n",
    "df_['Actual improvement'] = (df_.pos/df_.tot)/df_.truth - 1    \n",
    "\n",
    "\n",
    "# Graph the promised vs. actual improvement\n",
    "xlim = int(df_['Min Tech Index value'].max()) + 1\n",
    "xlim = xlim if xlim <= 20 else 20\n",
    "try:\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(\n",
    "        df_['Min Tech Index value'], \n",
    "        df_['Actual improvement'], \n",
    "        alpha = .5,\n",
    "        c = 'red'\n",
    "    )\n",
    "    plt.plot([0,xlim],[0,xlim],'k--',lw = 3)\n",
    "    plt.xlim([0,xlim])\n",
    "    plt.ylim([0,xlim * 2])\n",
    "    plt.xlabel('Min Tech Index value') \n",
    "    plt.ylabel('Actual improvement')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    for csc in df_.COMPANYCD.unique():\n",
    "        plt.plot(\n",
    "            df_[df_.COMPANYCD == csc]['Min Tech Index value'], \n",
    "            df_[df_.COMPANYCD == csc]['Actual improvement'], \n",
    "            lw = 2, \n",
    "        )\n",
    "    plt.plot([0,xlim],[0,xlim],'k--',lw = 3)\n",
    "    plt.xlim([0,xlim])\n",
    "    plt.ylim([0,xlim * 2])\n",
    "    plt.xlabel('Min Tech Index value') \n",
    "    plt.yticks([])\n",
    "    plt.ylabel('')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        folder + '/scatter.png', \n",
    "        facecolor = 'w', \n",
    "        transparent = False\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "*                              *\n",
      "*  No need to Re-Train models  *\n",
      "*                              *\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "with open(folder + '/last_train.pkl', 'rb') as f:\n",
    "    l_calmon = pickle.load(f)\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "mon_chng = pd.read_sql(\n",
    "    '''select count(distinct CALMONTHID)\n",
    "    from DEV_ANALYTICS.dbo.TECHINDEX_GENERAL_FEATURESET\n",
    "    where calmonthid>%s\n",
    "    ''' % (l_calmon),conn\n",
    ").iloc[0,0]\n",
    "conn.close()\n",
    "\n",
    "#if model hasn't been trained in last 3 months re-train it\n",
    "if (mon_chng>=3):\n",
    "    entitle(\"More than 3 months passed, Re-training models\")\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    #Get id of last 6 calmonths training data\n",
    "    t1 = pd.read_sql(\n",
    "        '''select min(calmonthid) as min_calmonthid, max(calmonthid) as max_calmonthid\n",
    "        from\n",
    "        (select calmonthid,\n",
    "        row_number() over(order by calmonthid desc) as rowid\n",
    "        from DEV_ANALYTICS.DBO.TECHINDEX_GENERAL_featureset(nolock)\n",
    "        group by calmonthid having count(distinct companycd)>20\n",
    "        ) x1 where x1.rowid<=%s\n",
    "        ''' % (m_cons), conn)\n",
    "    min_calmonthid = t1['min_calmonthid'][0]\n",
    "    max_calmonthid = t1['max_calmonthid'][0]\n",
    "\n",
    "    # Get a list of COMPANYCD\n",
    "    ccd = pd.read_sql(\n",
    "        '''select COMPANYCD, count(*) as TOTCOUNT, sum(LABEL) as POSCOUNT\n",
    "            from DEV_ANALYTICS.dbo.TECHINDEX_GENERAL_FEATURESET(nolock)\n",
    "            where calmonthid>=%s\n",
    "            group by COMPANYCD\n",
    "            order by COMPANYCD\n",
    "        ''' % (min_calmonthid), conn)\n",
    "    conn.close()\n",
    "    \n",
    "    \n",
    "    for i in range(len(ccd)):\n",
    "        companycd = ccd.iloc[i].COMPANYCD\n",
    "\n",
    "        # Skip countries with too few data points\n",
    "        if ccd.iloc[i].TOTCOUNT < (100*m_cons) : continue\n",
    "        if ccd.iloc[i].POSCOUNT < (10*m_cons) : continue\n",
    "        #Sample run for any country\n",
    "        #if companycd!='MD' : continue\n",
    "\n",
    "        baserate = ccd.iloc[i].POSCOUNT / ccd.iloc[i].TOTCOUNT\n",
    "\n",
    "        print(\"Building model for %s\"  % companycd)        \n",
    "\n",
    "        # Get the training data\n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        df = pd.read_sql(\n",
    "            '''\n",
    "                select *\n",
    "                from DEV_ANALYTICS.dbo.TECHINDEX_GENERAL_FEATURESET(nolock)\n",
    "                where COMPANYCD = '%s' and calmonthid>=%s\n",
    "                and ((Label = 1 and RowID <= %.0f)    -- pre-balancing in SQL, only\n",
    "                  or (Label = 0 and RowID <= %.0f))   -- for the gigantic countries\n",
    "            ''' % (\n",
    "                companycd, min_calmonthid,\n",
    "                maxsize/2/baserate/m_cons,\n",
    "                maxsize/2/(1-baserate)/m_cons\n",
    "            ),\n",
    "            conn\n",
    "        )\n",
    "        conn.close()\n",
    "\n",
    "        # Convert some features into numerical form\n",
    "        df['SICMAJORGRPCD'] = df.SICMAJORGRPCD.astype(float)\n",
    "        df['MONTH'] = df.CALMONTHID % 100 - 1\n",
    "        df['QMONTH'] = df.MONTH % 3\n",
    "        df['QUARTER'] = df.MONTH // 3\n",
    "\n",
    "        # Balance the training data (if necessary and possible) \n",
    "        # by throwing away negative cases\n",
    "        pos = sum(df.Label)\n",
    "        negs_needed = max([pos, 1000 - pos])\n",
    "        if negs_needed > len(df) - pos:\n",
    "            negs_needed = len(df) - pos\n",
    "\n",
    "        df = df[df.Label == 1].append(df[df.Label == 0].sample(n = negs_needed))    \n",
    "\n",
    "        # Drop the columns that should never be used as features\n",
    "        X = df.drop([\n",
    "            'CALMONTHID', 'COMPANYCD', 'DM_ULT_DUN', 'Label', 'RowID'\n",
    "        ], 1).fillna({\n",
    "            'EmployeesTotalFinal' : -1,\n",
    "            'SICMAJORGRPCD' : 100\n",
    "        }).fillna(-9999)\n",
    "\n",
    "        # Store the labels (bought or didn't buy) separately\n",
    "        y = df['Label']\n",
    "\n",
    "        # If there's only one kind of label, skip the country entirely\n",
    "        if len(y.unique()) < 2 or sum(y) < 10: continue\n",
    "\n",
    "        imp = permutation_importances(X, y)\n",
    "        print('Found %d important features.' % sum(i_ > 1e-10 for i_ in imp))\n",
    "\n",
    "        # Get a random selection of hyperparameter sets for optimization\n",
    "        pgrid = ParameterSampler(\n",
    "            {\n",
    "                \"estimator__max_depth\"         : geom(.08, 1),\n",
    "                \"estimator__max_features\"      : uniform(0, 1),\n",
    "                \"estimator__min_samples_leaf\"  : np.logspace(0.2, 2.7, 100).astype(int),\n",
    "                \"estimator__min_samples_split\" : np.logspace(.31, 3, 100).astype(int),\n",
    "                \"estimator__n_estimators\"      : [300],\n",
    "                \"selector__k\"                  : randint(5, X.shape[1])\n",
    "            }, \n",
    "            n_iter = gridsize\n",
    "        )\n",
    "\n",
    "        # If optimal hyperparameters were previously recorded, add them back in\n",
    "        try:\n",
    "            with open(folder + 'params_%s.pkl' % companycd, 'rb') as f:\n",
    "                oldp = pickle.load(f)\n",
    "            pgrid = [p for p in pgrid] + [oldp]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Clean up the format of the hyperparameter sets for GridSearchCV\n",
    "        pgrid = [\n",
    "            {\n",
    "                xx:[xv] \n",
    "                for (xx,xv) in zip(p.keys(), p.values())\n",
    "            } \n",
    "            for p in pgrid\n",
    "        ]     \n",
    "\n",
    "        # Use cross-validation to find the best set of hyperparameters\n",
    "        # (this is where all the time is spent)\n",
    "        pipe = Pipeline([\n",
    "            ('selector'  , SelectKBest(scores(imp))),\n",
    "            ('estimator' , RandomForestClassifier())\n",
    "        ])\n",
    "        gs = GridSearchCV(\n",
    "            pipe, \n",
    "            param_grid = pgrid,\n",
    "            n_jobs = -2,          # Use all but one of the computer's cores\n",
    "            scoring = 'roc_auc',  # Judge by AUC\n",
    "    #        cv = StratifiedKFold(3, shuffle = True),\n",
    "            verbose = 3           # Report progress along the way\n",
    "        )\n",
    "        gs.fit(X, y)\n",
    "        clf = gs.best_estimator_\n",
    "        print(\"Best cross-validation AUC: %.4f\\n\\n\" % (gs.best_score_))\n",
    "\n",
    "        # Save the best model and hyperparameters\n",
    "        with open(folder + 'params_%s.pkl' % companycd, 'wb') as f:\n",
    "            pickle.dump(gs.best_params_, f)\n",
    "\n",
    "        with open(folder + 'clf_%s.pkl' % companycd, 'wb') as f:\n",
    "            pickle.dump(clf, f)\n",
    "\n",
    "\n",
    "    with open(folder + 'last_train.pkl', 'wb') as f:\n",
    "        pickle.dump(max_calmonthid, f)\n",
    "        \n",
    "else: entitle(\"No need to Re-Train models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "*                                        *\n",
      "*  Building next month's predictions...  *\n",
      "*                                        *\n",
      "******************************************\n"
     ]
    }
   ],
   "source": [
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# Get the CALMONTHID of the latest unlabeled features\n",
    "calmon = pd.read_sql(\n",
    "    '''select max(CALMONTHID)\n",
    "    from DEV_ANALYTICS.dbo.TECHINDEX_GENERAL_FEATURESET_unlabeled\n",
    "    ''',conn\n",
    ").iloc[0,0]\n",
    "\n",
    "\n",
    "\n",
    "# Get a list of all COMPANYCD's\n",
    "ccd = pd.read_sql(\n",
    "    '''select COMPANYCD\n",
    "    from DEV_ANALYTICS.DBO.TECHINDEX_GENERAL_featureset_unlabeled   \n",
    "    where CALMONTHID = %d\n",
    "    group by COMPANYCD\n",
    "    order by COMPANYCD\n",
    "    ''' % calmon, conn\n",
    ")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "entitle(\"Building next month's predictions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR\n",
      "AT\n",
      "Austria\n",
      "Most important features:\n",
      "* DAYSSINCELASTTRANS\n",
      "* TRANSCOUNT\n",
      "* PRODUCTCOUNT\n",
      "\n",
      "Scoring 8803 end users, with explanations for the top 1760 scores... AU\n",
      "Australia\n",
      "Most important features:\n",
      "* EmployeesTotalFinal\n",
      "* TRANSCOUNT\n",
      "* EXTENDEDSALES\n",
      "\n",
      "Scoring 208744 end users, with explanations for the top 10000 scores... BE\n",
      "Belgium\n",
      "Most important features:\n",
      "* EmployeesTotalFinal\n",
      "* TRANSCOUNT\n",
      "* DAYSSINCELASTTRANS\n",
      "\n",
      "Scoring 33813 end users, with explanations for the top 6762 scores... BR\n",
      "Brazil\n",
      "Most important features:\n",
      "* DAYSSINCELASTTRANS\n",
      "* EmployeesTotalFinal\n",
      "* TRANSCOUNT3M\n",
      "\n",
      "Scoring 20167 end users, with explanations for the top 4033 scores... CH\n",
      "Switzerland\n",
      "Most important features:\n",
      "* TRANSCOUNT\n",
      "* EmployeesTotalFinal\n",
      "* PRODUCTCOUNT\n",
      "\n",
      "Scoring 34687 end users, with explanations for the top 6937 scores... CL\n",
      "Chile\n",
      "Most important features:\n",
      "* DAYSSINCELASTTRANS\n",
      "* TRANSCOUNT\n",
      "* TRANSCOUNT6M\n",
      "\n",
      "Scoring 4773 end users, with explanations for the top 954 scores... CO\n",
      "Colombia\n",
      "Most important features:\n",
      "* EmployeesTotalFinal\n",
      "* DAYSSINCELASTTRANS\n",
      "* EXTENDEDSALES\n",
      "\n",
      "Scoring 7143 end users, with explanations for the top 1428 scores... CR\n",
      "DE\n",
      "Germany\n",
      "Most important features:\n",
      "* EmployeesTotalFinal\n",
      "* TRANSCOUNT\n",
      "* DAYSSINCELASTTRANS\n",
      "\n",
      "Scoring 208655 end users, with explanations for the top 10000 scores... EH\n",
      "China\n",
      "Most important features:\n",
      "* TRANSCOUNT6M\n",
      "* EXTENDEDSALES3M\n",
      "* DAYSSINCELASTTRANS\n",
      "\n",
      "Scoring 2296 end users, with explanations for the top 459 scores... ES\n",
      "Spain\n",
      "Most important features:\n",
      "* EmployeesTotalFinal\n",
      "* DAYSSINCELASTTRANS\n",
      "* TRANSCOUNT\n",
      "\n",
      "Scoring 57426 end users, with explanations for the top 10000 scores... FR\n",
      "France\n",
      "Most important features:\n",
      "* EmployeesTotalFinal\n",
      "* TRANSCOUNT\n",
      "* PRODUCTCOUNT\n",
      "\n",
      "Scoring 135523 end users, with explanations for the top 10000 scores... FT\n",
      "Canada\n",
      "Most important features:\n",
      "* TRANSCOUNT\n",
      "* EmployeesTotalFinal\n",
      "* DAYSSINCELASTTRANS\n",
      "\n",
      "Scoring 186144 end users, with explanations for the top 10000 scores... HK\n",
      "Hong Kong\n",
      "Most important features:\n",
      "* EmployeesTotalFinal\n",
      "* DAYSSINCELASTTRANS\n",
      "* TRANSCOUNT\n",
      "\n",
      "Scoring 4907 end users, with explanations for the top 981 scores... HU\n",
      "IA\n",
      "India\n",
      "Most important features:\n",
      "* DAYSSINCELASTTRANS\n",
      "* TRANSCOUNT\n",
      "* EmployeesTotalFinal\n",
      "\n",
      "Scoring 33690 end users, with explanations for the top 6738 scores... ID\n",
      "Indonesia\n",
      "Most important features:\n",
      "* DAYSSINCELASTTRANS\n",
      "* EmployeesTotalFinal\n",
      "* TRANSCOUNT\n",
      "\n",
      "Scoring 1955 end users, with explanations for the top 391 scores... IT\n",
      "Italy\n",
      "Most important features:\n",
      "* EmployeesTotalFinal\n",
      "* MONTH\n",
      "* TRANSCOUNT\n",
      "\n",
      "Scoring 125407 end users, with explanations for the top 10000 scores... MD\n",
      "United States of America\n",
      "Most important features:\n",
      "* EmployeesTotalFinal\n",
      "* TRANSCOUNT\n",
      "* EXTENDEDSALES\n",
      "\n",
      "Scoring 1482714 end users, with explanations for the top 10000 scores... MX\n",
      "Mexico\n",
      "Most important features:\n",
      "* DAYSSINCELASTTRANS\n",
      "* TRANSCOUNT\n",
      "* EmployeesTotalFinal\n",
      "\n",
      "Scoring 33892 end users, with explanations for the top 6778 scores... MY\n",
      "Malaysia\n",
      "Most important features:\n",
      "* TRANSCOUNT\n",
      "* DAYSSINCELASTTRANS\n",
      "* EXTENDEDSALES4Q\n",
      "\n",
      "Scoring 14130 end users, with explanations for the top 2826 scores... NL\n",
      "Netherlands\n",
      "Most important features:\n",
      "* EmployeesTotalFinal\n",
      "* TRANSCOUNT\n",
      "* DAYSSINCELASTTRANS\n",
      "\n",
      "Scoring 67793 end users, with explanations for the top 10000 scores... NZ\n",
      "New Zealand\n",
      "Most important features:\n",
      "* TRANSCOUNT\n",
      "* EmployeesTotalFinal\n",
      "* EXTENDEDSALES\n",
      "\n",
      "Scoring 63742 end users, with explanations for the top 10000 scores... PE\n",
      "Peru\n",
      "Most important features:\n",
      "* DAYSSINCELASTTRANS\n",
      "* TRANSCOUNT6M\n",
      "* TRANSCOUNT\n",
      "\n",
      "Scoring 8046 end users, with explanations for the top 1609 scores... SE\n",
      "Sweden\n",
      "Most important features:\n",
      "* DAYSSINCELASTTRANS\n",
      "* TRANSCOUNT\n",
      "* PRODUCTCOUNT\n",
      "\n",
      "Scoring 32187 end users, with explanations for the top 6437 scores... SG\n",
      "Singapore\n",
      "Most important features:\n",
      "* TRANSCOUNT\n",
      "* EmployeesTotalFinal\n",
      "* EXTENDEDSALES\n",
      "\n",
      "Scoring 19856 end users, with explanations for the top 3971 scores... TH\n",
      "Thailand\n",
      "Most important features:\n",
      "* DAYSSINCELASTTRANS\n",
      "* TRANSCOUNT\n",
      "* PRODUCTCOUNT\n",
      "\n",
      "Scoring 1203 end users, with explanations for the top 240 scores... UK\n",
      "United Kingdom\n",
      "Most important features:\n",
      "* TRANSCOUNT\n",
      "* EmployeesTotalFinal\n",
      "* DAYSSINCELASTTRANS\n",
      "\n",
      "Scoring 313236 end users, with explanations for the top 10000 scores... UY\n"
     ]
    }
   ],
   "source": [
    "# Before the first batch of predictions, we will generate a header\n",
    "header = True\n",
    "\n",
    "# Loop over all COMPANYCD\n",
    "for i in range(len(ccd)):\n",
    "\n",
    "    companycd = ccd.iloc[i].COMPANYCD\n",
    "    start = datetime.now()\n",
    "\n",
    "    print(companycd)\n",
    "\n",
    "    # Try to find a classifier for this COMPANYCD\n",
    "    try:\n",
    "        with open(folder + '/clf_%s.pkl' % companycd, 'rb') as f:\n",
    "            clf = pickle.load(f)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # Adjust the classifier to use all CPU cores\n",
    "    clf.named_steps['estimator'].set_params(n_jobs = -1)\n",
    "\n",
    "    # Try to find a standardizing function for this COMPANYCD\n",
    "    try:\n",
    "        with open(folder + '/spf_%s.pkl' % companycd, 'rb') as f:\n",
    "            spf = pickle.load(f)\n",
    "    # If none exists, the TECHINDEX_VALUE will simply be the percentile (as a decimal)\n",
    "    except:\n",
    "        spf = (lambda x : 1 - x)\n",
    "    \n",
    "    conn = pyodbc.connect(conn_str)\n",
    "\n",
    "    # Report the English description of the current COMPANYCD\n",
    "    try:\n",
    "        print(\n",
    "            pd.read_sql(\n",
    "                '''\n",
    "                select VANITY_NAME as VN\n",
    "                from BIC_REFERENCE.dbo.UI_REGION\n",
    "                where COMPANYCD = '%s'\n",
    "                ''' % companycd,\n",
    "                conn\n",
    "            ).VN[0]\n",
    "        )\n",
    "    except:\n",
    "        print('(Country name unknown.)')\n",
    "\n",
    "    # Get the features for this COMPANYCD\n",
    "    df = pd.read_sql(\n",
    "        '''\n",
    "            select *\n",
    "            from DEV_ANALYTICS.dbo.TECHINDEX_GENERAL_FEATURESET_unlabeled\n",
    "            where COMPANYCD = '%s' and CALMONTHID = %d\n",
    "        ''' % (companycd, calmon),\n",
    "        conn\n",
    "    )\n",
    "    conn.close()\n",
    "    \n",
    "    # Skip this COMPANYCD if no data is found\n",
    "    if len(df) < 1: \n",
    "        continue\n",
    "\n",
    "    # Prepare the output dataframe\n",
    "    ret = df.assign(\n",
    "        LEVEL = 'GENERAL'\n",
    "    ).assign(\n",
    "        GROUP = ''\n",
    "    )[['LEVEL', 'CALMONTHID', 'COMPANYCD', 'DM_ULT_DUN', 'GROUP']]\n",
    "\n",
    "    # Convert some features into numerical form\n",
    "    df['SICMAJORGRPCD'] = df.SICMAJORGRPCD.astype(float)\n",
    "    df['MONTH'] = df.CALMONTHID % 100 - 1\n",
    "    df['QMONTH'] = df.MONTH % 3\n",
    "    df['QUARTER'] = df.MONTH // 3\n",
    "    \n",
    "    # Drop the columns that should never be used as features\n",
    "    # and fill missing values\n",
    "    X = df.drop([\n",
    "        'CALMONTHID', 'COMPANYCD', 'DM_ULT_DUN', 'Label', 'RowID'\n",
    "    ], 1).fillna({\n",
    "        'DAYSSINCELASTTRANS' : 750,\n",
    "        'EmployeesTotalFinal' : -1,\n",
    "        'SICMAJORGRPCD' : 100\n",
    "    }).fillna(-9999)\n",
    " \n",
    "    # Report the most important features that were found when the model was trained\n",
    "    print('Most important features:')\n",
    "    for pair in sorted(\n",
    "        zip(X.columns, clf.named_steps['selector'].score_func.imp), \n",
    "        key = lambda pair: -pair[1]\n",
    "    )[:3]:\n",
    "        print('*', pair[0])\n",
    "    \n",
    "    print('\\nScoring %d end users,' % (len(df)), end = ' ')\n",
    "\n",
    "    # Get the scores from the classifier and use them \n",
    "    # with the standardizer to produce the TECHINDEX_VALUEs\n",
    "    ret['RAWSCORE'] = clf.predict_proba(X)[:,1]\n",
    "    ret['TECHINDEX_VALUE'] = (\n",
    "        ret.RAWSCORE.rank(axis = 0, method = 'min', ascending = False) / len(ret.RAWSCORE)\n",
    "    ).apply(spf)\n",
    "\n",
    "    # Calculate how many explanations to be produced:\n",
    "    # The top 20%, up to a maximum of 10K end users\n",
    "    score_threshold = ret.RAWSCORE.quantile(q = 0.8)\n",
    "    rank = ret.rank(ascending = False).RAWSCORE\n",
    "    index_top = (rank <= 10000) & (rank <= len(X) // 5)   \n",
    "\n",
    "    print('with explanations for the top %d scores...' % sum(index_top), end = ' ')\n",
    "    \n",
    "    # Get the explanations\n",
    "    if sum(index_top) > 0:\n",
    "        pred_exp = tree_explainer.predict_explain(\n",
    "            clf.named_steps['estimator'], \n",
    "            pd.DataFrame(\n",
    "                clf.named_steps['selector'].transform(X[index_top]),\n",
    "                columns = [\n",
    "                    str(n) \n",
    "                    for n,b in enumerate(clf.named_steps['selector'].get_support()) \n",
    "                    if b\n",
    "                ]\n",
    "            ),\n",
    "            confidence = 0.67\n",
    "        )\n",
    "\n",
    "    # Copy the explanations into the output dataframe\n",
    "    ret['REASON1CD'] = ''\n",
    "    ret['REASON2CD'] = ''\n",
    "    if sum(index_top) > 0:\n",
    "        ret.loc[index_top, 'REASON1CD'] = pred_exp.REASON1.values\n",
    "        ret.loc[index_top, 'REASON2CD'] = pred_exp.REASON2.values    \n",
    "    \n",
    "    # Save the output to disk \n",
    "    # If it's the first batch, overwrite the file and use a header\n",
    "    # Otherwise, append with no header\n",
    "    ret.to_csv(outfolder + 'TECHINDEX.csv', header = header,\n",
    "               mode = ('w' if header else 'a'), index = False)\n",
    "    header = False\n",
    "    \n",
    "    \n",
    "# Output a file of reason codes and reasons\n",
    "header = True\n",
    "for l, s in [('l', 'small'), ('m', 'moderate'), ('h', 'large')]:\n",
    "    for i, c in enumerate(X.columns):\n",
    "        with open(\n",
    "            outfolder + 'TECHINDEX_REASONS.csv', \n",
    "            'w' if header else 'a'\n",
    "        ) as f:\n",
    "            if header: f.write('LEVEL,REASONCD,REASON\\n')\n",
    "            f.write('GENERAL,%d%s,%s %s\\n' % (i,l,s,feature_english(c)))\n",
    "        header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3110932 entries, 0 to 3110931\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   LEVEL            object \n",
      " 1   CALMONTHID       int64  \n",
      " 2   COMPANYCD        object \n",
      " 3   DM_ULT_DUN       object \n",
      " 4   GROUP            float64\n",
      " 5   RAWSCORE         float64\n",
      " 6   TECHINDEX_VALUE  float64\n",
      " 7   REASON1CD        object \n",
      " 8   REASON2CD        object \n",
      "dtypes: float64(3), int64(1), object(5)\n",
      "memory usage: 213.6+ MB\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv(outfolder + 'TECHINDEX.csv', dtype={'DM_ULT_DUN':'str'}).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine1 = create_engine('mssql://uschwsql1056d/DEV_ANALYTICS?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server',\\\n",
    "                       fast_executemany=True)\n",
    "\n",
    "pd.read_csv(outfolder + 'TECHINDEX_REASONS.csv').to_sql('TECHINDEX_GENERAL_LOAD_REASONS', con=engine1,\n",
    "                                                        schema='dbo', if_exists='append',\n",
    "                                                        chunksize=100000, index=False)\n",
    "pd.read_csv(outfolder + 'TECHINDEX.csv',\n",
    "            dtype={'DM_ULT_DUN':'str'}).to_sql('TECHINDEX_GENERAL_LOAD', con=engine1,\n",
    "                                                        schema='dbo', if_exists='append',\n",
    "                                                        chunksize=100000, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
